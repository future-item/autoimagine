<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <meta http-equiv="X-UA-Compatible" content="ie=edge">
    <title>Project Title - Home</title>
    <link rel="stylesheet" href="styles.css"> <!-- Optional: Link to an external CSS file for styling -->

    <title>Center Video</title>
    <style>
      /* 水平居中视频 */
      body {
        text-align: center;  /* 水平居中 */
      }
  
      video {
        width: 80%;
        max-width: 800px;
        margin-top: 50px;  /* 可选：给视频上方添加一些空白 */
      }
    </style>
    <title>Enhancing Visual Reasoning with Autonomous Imagination in Multimodal Large Language Models</title>
    
</head>
    
<body>

    <header>
        <h1>Enhancing Visual Reasoning with Autonomous Imagination in Multimodal Large Language Models</h1>
        <p><em>By: Author Name</em></p>
    </header>

    <section id="abstract">
        <h2>Abstract</h2>
        <p>There have been recent efforts to extend the Chain-of-Thought (CoT) paradigm to Multimodal Large Language Models (MLLMs) by finding visual clues in the input scene, advancing the visual reasoning ability of MLLMs. However, current approaches are specially designed for the tasks where clue finding plays a major role in the whole reasoning process, leading to the difficulty in handling complex visual scenes where clue finding does not actually simplify the whole reasoning task. To deal with this challenge, we propose a new visual reasoning paradigm enabling MLLMs to autonomously modify the input scene to new ones based on its reasoning status, such that CoT is reformulated as conducting simple closed-loop decision-making and reasoning steps under a sequence of “imagined” visual scenes, leading to natural and general CoT construction. To implement this paradigm, we introduce a novel plug-and-play “imagination space”, where MLLMs conduct visual modifications through operations like focus, ignore, and transform based on their native reasoning ability without specific training. We validate our approach through a benchmark spanning dense counting, simple jigsaw puzzle solving, and object placement, challenging the reasoning ability beyond clue finding. The results verify that while existing techniques fall short, our approach enables MLLMs to effectively reason step by step through autonomous imagination.</p>
    </section>

    <section id="content">
        <h2>Project Content</h2>
        <p>to be completed.</p>
    </section>
        
    <video width="600" controls>
      <source src="https://github.com/future-item/autoimagine/jigsaw puzzle visualization.mp4" type="video/mp4">
      Your browser does not support the video tag.
    </video>
    
    <section id="references">
        <h2>References</h2>
        <ul>
            <li>Author1, Author2, "Title of the Paper," Journal, Year.</li>
            <li>Author1, Author2, "Title of the Paper," Conference, Year.</li>
        </ul>
    </section>

    <footer>
        <p>Find more details on the <a href="https://github.com/your-username/your-repository" target="_blank">GitHub repository</a>.</p>
    </footer>

</body>
</html>
